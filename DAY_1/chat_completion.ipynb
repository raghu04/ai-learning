{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIqdkjL_phti",
    "outputId": "75984b97-dc51-4e12-89c2-18a118fddc65"
   },
   "outputs": [],
   "source": [
    "%pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvCh3auFphtl"
   },
   "source": [
    "## Local Inference on GPU\n",
    "Model page: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n",
    "\n",
    "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUOH2BM7phtm"
   },
   "source": [
    "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144,
     "referenced_widgets": [
      "770af76fd24242d0bf82b40be199bccc",
      "5bbacdedf2af43ffa8589eac2bdf02f8",
      "7290e79b13e24e84bcda205ba50242ed",
      "f4e3402135a8497185fa91906fcd1839",
      "6cb3def83e6a4e1ba17b170b62d871d0",
      "91dc739e870a46019feb64e9b9def968",
      "004e83a1590245d0942573851a221823",
      "cc3ba6711b8c43fd8b49539933a67118",
      "c4d37944f26f47589e6d499c30bb9d76",
      "9634a8716cfe48c7aa2702ce7cd99e79",
      "3bfbf3f6860742a6904975ed8df36f63",
      "567cd997775d41dbac7625e9a5c650de",
      "26d4f506e3ba4d978a2489373649c183",
      "f4fe252a655e4a7288d432b87dcba08b",
      "221b5a5b8d9b4ad5bc946de2a14a8a6a",
      "c2612832045542129db216a6a1d7daa6",
      "ec9054801f974a0f99ce09e05a6ef3aa",
      "36b3868913154d3589287f139870c563",
      "cedbb2c77688436ba1ceccf197d69072",
      "72306af1ed824559ba42e19fd5ae7329"
     ]
    },
    "id": "wnP8CfgJphtn",
    "outputId": "190fa08a-a6e8-4969-8fc7-323d838a5fa5"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flMXL9GBphto"
   },
   "source": [
    "## Remote Inference via Inference Providers\n",
    "Ensure you have a valid **HF_TOKEN** set in your environment, running this may bill your account above the free tier.\n",
    "The following Python example shows how to run the model remotely on HF Inference Providers, using the **auto** provider setting (automatically selects an available inference provider)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U4QzsKuphto",
    "outputId": "0a850603-9771-40c6-c33b-76b1780e3e9d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"auto\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \" The capital of France is Paris. It is one of the most famous cities in the world, known for its rich history, art, culture, and landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris is also the political, economic, and cultural center of France.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How confident are you?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"  I am a model and my responses are based on the data I have been trained on. I strive to provide accurate and helpful information, but I don't have personal feelings or emotions. I don't have the ability to be confident or uncertain. I simply provide the information I have been programmed to know.\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "print('---')\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_-BAPpVppsN",
    "outputId": "52faf39e-29e1-42d0-8932-c03e0c42e4e2"
   },
   "outputs": [],
   "source": [
    "# prompt: Can you do another chat completions call but this time â€”Â show log probabilities of top 10 tokens for each token inferred. And then the final message\n",
    "import math, pandas as pd\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")\n",
    "# print(completion.choices[0].message)\n",
    "# completion.choices[0].logprobs\n",
    "\n",
    "# logprobs = completion.choices[0].logprobs     # shortcut\n",
    "# records = []\n",
    "\n",
    "# for idx, logprob_item in enumerate(logprobs.content):\n",
    "#   tok = logprob_item.token\n",
    "#   lp = logprob_item.logprob\n",
    "#   top = logprob_item.top_logprobs\n",
    "\n",
    "#   chosen = f\"{tok} ({math.exp(lp):.3f})\"\n",
    "\n",
    "#   # alts is now a list of Logprob objects, not a dictionary\n",
    "#   alts_fmt = []\n",
    "\n",
    "# for alt_logprob_item in top:\n",
    "#   alts_fmt.append(f\"{alt_logprob_item.token} ({math.exp(alt_logprob_item.logprob):.3f})\")\n",
    "\n",
    "# alts_fmt += [''] * (5 - len(alts_fmt))          # pad to 5\n",
    "\n",
    "# records.append([idx, chosen, *alts_fmt])\n",
    "\n",
    "\n",
    "for idx, (tok, lp, top) in enumerate(zip(\n",
    "        logprobs.tokens,\n",
    "        logprobs.token_logprobs,\n",
    "        logprobs.top_logprobs)):\n",
    "\n",
    "    chosen = f\"{tok} ({math.exp(lp):.3f})\"\n",
    "    alts = sorted(top.items(), key=lambda kv: kv[1], reverse=True)[:5]\n",
    "    alts_fmt = [f\"{t} ({math.exp(lp_):.3f})\" for t, lp_ in alts]\n",
    "    alts_fmt += [''] * (5 - len(alts_fmt))          # pad to 5\n",
    "\n",
    "    records.append([idx, chosen, *alts_fmt])\n",
    "\n",
    "df = pd.DataFrame(records,\n",
    "                  columns=[\"Idx\", \"Chosen (p)\", \"Alt-1\", \"Alt-2\",\n",
    "                           \"Alt-3\", \"Alt-4\", \"Alt-5\"])\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "enPtKOJY73MT",
    "outputId": "ae1a5182-1a55-4a96-ad81-c5f57cc9c69c"
   },
   "outputs": [],
   "source": [
    "# prompt: Write code that can redner the response from LLM, colour code each token/word based on probability. Higher to lower colours: [ #FFFFFF, #dbdbdb, #adadac, #696868, #333331, #000000]   Prompt, role=user \"Write 3 paragraphs about Paris\"\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def render_colored_text(completion):\n",
    "    \"\"\"\n",
    "    Renders the text response from an LLM, coloring each token based on its probability.\n",
    "\n",
    "    Args:\n",
    "        completion: The completion object from the LLM client with logprobs enabled.\n",
    "    \"\"\"\n",
    "    logprobs = completion.choices[0].logprobs\n",
    "    if not logprobs or not logprobs.token_logprobs:\n",
    "        print(\"Log probabilities not available in the completion.\")\n",
    "        print(completion.choices[0].message.content)\n",
    "        return\n",
    "\n",
    "    # Define color mapping for probabilities (higher to lower)\n",
    "    # Mapping log_prob to a 0-1 range based on min/max log_probs\n",
    "    colors = [\"#000000\", \"#333331\", \"#696868\", \"#adadac\", \"#dbdbdb\", \"#FFFFFF\"] # reversed for lower prob darker\n",
    "    min_log_prob = min(logprobs.token_logprobs)\n",
    "    max_log_prob = max(logprobs.token_logprobs)\n",
    "    log_prob_range = max_log_prob - min_log_prob\n",
    "\n",
    "    html_output = \"\"\n",
    "    for token, log_prob in zip(logprobs.tokens, logprobs.token_logprobs):\n",
    "        # Normalize log_prob to a 0-1 range for color mapping\n",
    "        if log_prob_range > 0:\n",
    "            normalized_prob = (log_prob - min_log_prob) / log_prob_range\n",
    "        else:\n",
    "            normalized_prob = 0.5 # Default to middle color if all log_probs are the same\n",
    "\n",
    "        # Map normalized_prob to a color index\n",
    "        color_index = int(normalized_prob * (len(colors) - 1))\n",
    "        color = colors[color_index]\n",
    "\n",
    "        # Encode HTML entities for special characters in tokens\n",
    "        import html\n",
    "        escaped_token = html.escape(token)\n",
    "\n",
    "        html_output += f'<span style=\"color: {color};\">{escaped_token}</span>'\n",
    "\n",
    "    display(HTML(html_output))\n",
    "\n",
    "# Render the response with colors\n",
    "print(\"Rendering response with color coding based on token probability:\")\n",
    "\n",
    "completion_paris = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write 3 paragraphs about Paris\"\n",
    "        }\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")\n",
    "\n",
    "render_colored_text(completion_paris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rrMYO4oFX6q"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3m9c7ve7yfK"
   },
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "oFwluWSrsXQ0",
    "outputId": "857110a3-df8c-4834-96a9-3fec7724e7c9"
   },
   "outputs": [],
   "source": [
    "# prompt: Do same question \"What is capital of france\" with chat completion â€” allow controlling other factors like output tokens, temperature\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1,  # Set the maximum number of output tokens\n",
    "    temperature=0 # Set the temperature for creativity (0.0 to 1.0)\n",
    ")\n",
    "\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JgAfHYkfbnw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
